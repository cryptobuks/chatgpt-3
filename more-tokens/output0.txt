Presenter-A: Screen 20. So actually is either more than we usually use with everything that is.
Presenter-A:  Okay. So I have to bigquery migration. That's a kind of popular topic and popular use case that some of us probably have already faced and some of us going to face the future. Hive is Data Warehouse developed by Apache, which has quite popular among companies, Bigquery is kind of counterpart for hive that Google created and provides in GCP and which extends and adds a lot of more features. And usually while working with clients project or regarding migration, we have cases where we have to migrate data from, I have to be career. So of course, the scenarios and cases going to be a lot.
Presenter-A:  Based on the specific project but we will try to talk to a little bit general about use cases trying to cover most of the issues that you can face during this kind of project. And what's the possible?
Presenter-A:  Strategies Technologies for instep that you can use. So let's start by looking at general architecture for our case, How we're going to do this, migration. First of all, we are going to do some assumptions. So specifically, we will assume that net forking part regarding connection is already sold. So we don't have any connection issues. There are connection from our source environment to destination environment. Also there is no permission issues so we can go ahead. As you can see here, the brief architecture source environment, which in our cases, on PREM environment, we have hive database and it starts for me, then it goes to some preparation work first, preparation work.
Presenter-A:  After that, we use dataproc service as kind of staging environment and also compute for performing them migration tasks. We use this CP software later. We will talk a little bit more detail about this one. To get data from our source and copy to cloud storage after that, we have for us to for preparation work. And after that, we load the data into staging tables in Bigquery we again, do some modifications well loading data from staging to target tables. And after all of these we can say we that we have down the migration
Presenter-A:  So moving forward, let's start talking about the source. So as mentioned in our case, the source is Apache. Have a database so we can have Two Types of Tables. There are internal and external life tables. Each one has its specifications. As you can see regard, if you are working with internal tables,
Presenter-A:  Hive moves, stable data to data warehouse directory, and you have acid transactions enabled? It supported from for Truecade their other features like query result, caching works in this case. I like, in case of external type tables, and also dropping delete table data and metadata, like, in case of external table, where you even when delete table under link data still remains, and there is no support for truncate or acid transactions. so there could be different formats for source data, and each format actually has its specifications
Presenter-A:  And pros and consent while working with this format. So there I have mentioned here, the six main formats that usually you can face while working with Hive a CSV. Most common format just song, XML overall parquet and orc formats are more regarding the internal tables and those are preferred actually when you are important data to be query because those are usually in compressed format and makes it quicker and easier to work. Also, those usually have schema in file as well. So you don't need to worry about the schema of the destination table too much with the content for being able to
00:05:00
Presenter-A:  The data. So if we are looking, for example, at target, we there are some suggestions and requirements that you need to follow while working with
Presenter-A:  migration, this kind of format like Keep Record size to 50 megabytes or less and they're only specific compression Codex for data blocks in market files that supported by Bigquery. So you need to confirm that you follow those rules. And after that, you can go ahead and do the migration. So I have added all these details in the document that later I will share with you other. You can see all the details about each format
Presenter-A:  Which will be careful, I believe. So, moving forward, let's talk about first fats for Preparation block. So when we're working with internal tables, where we have acid operations enabled, Who may need to deal with Table compaction. So when you have this feature enabled during the each transaction when you
Presenter-A: Are Replace AD or remove something from tables. You are going to end up with having extra files in a Hive data warehouse. Directories. A lot of files going to be edit during those transactions and have used to manage operations this way.
Miguel Izaga: Up.
Presenter-A: So after those AC IDE operations, if you try to read something from result table, it's goes through all those small files to give you the last result of the data, but it it's kind of problematic for us because when we are going to migrate this format, it's going to be problematic for Bigquery to import that format is not supported.
Presenter-A:  And in our case we can do one thing that's Compaction. Compaction is specific operation that you can run on source cable which will combine all those small files that I was talking about and create one base file which will make it much easier to copy to destination directories and eventually load to bigquery tables. Without any issues As I have talked previously some files like CSV. They don't have information about schema in data file itself.
Presenter-A:  Some loading features like PQ load have options to auto detect schema. Well, you're loading data to be query table, but that's not ideal option and it has its limitations and sometimes it cannot ideally understand and detects schema based on the content file and in those four. Avoiding Issues and Any confusions. You may need to have your separate, schema dump file before starting loading process, and creating those files can be one of the preparation books.
00:10:00
Presenter-A:  Also, you need to collect more information about about source tables. Like Do you have partitions class source in source, tables? Are you going to have that in a destination table or information? Like that's source name, and destination name of the tables are the same or you're going to rename in the destination site. So all of this information you can keep in one config file.
Presenter-A:  Which will be part of the migration and I will, we will make it automatic later. I will show you some examples. And also one preparation work You need to collect information about existing processes and jobs that are running on source site because adding extra load by performing the migration called Create some conflicts with existing processing and jobs. And you may need, for example, some freeze time that you need to consider and plan. So all of these will be parts of the preparation work one.
Presenter-A:  So moving forward, this is just an example of config file, formats that in our case, we are going to use. So very simple. You just have table name bigquery data set name table name on destination site. The file format You need to preserve the schema file if you have such otherwise it will go by default scenarios of loading. And also you can specify if you have partition flag or not. Going forward. Let's talk about the next step, which is coping the actual data from source to GCS market, GCS bracket going to be a staging area, which going to be used. For actual load to Bigquery.
Presenter-A:  So there are different research. IT scenarios and tools that can be used for performing this operation. Most popular scenarios are the ones that listed in this screen. So if you are working with HDFS data, you can use one of the four tools mentioned here. GSU team to is not preferable to because in this case, it means that you need to copy from HDFS to local file system first and from there, copy to GCS which means if you have huge amount of data, it's going to be problematic to keep all the information in all the data and one file system before moving to
Presenter-A:  GCs, so it's mostly preferred option if you are working with a small amount of content. This CP tool is open open source software. Which can use can be used to copy data directly from HDF between HDFS and GCS.
Presenter-A:  So there are two methods to models push model and pool model. So in our scenario we will talk more about a pool model because it has more advantages, but there's also push model. So what we mean by push and pull models, DCP tool needs to be installed in the environment where you are going to run actual DCP command. So if you're going to push model, your you need to install the cp2 in directly, in source server, where you have your HDFS data located and around Command Director there, assuming that you have all the connection that access points enable,
Presenter-A:  And around the jobs to copy data from source environment to GCS Bucket. The one important disadvantages that if you are working with huge amount of data, Putting extra pressure on sourcing environments, or servers can affect existing jobs and existing processes that are running. So you need to consider that pool model. In other hand means that you are going to use third environment. Like we did, we are using data protein environment and there we are running DCP tool which will establish connection between source environment where actual HDFS data located and the destination environment GCS.
00:15:00
Presenter-A:  So it will run in dataproc and will not affect any. How To source environment which are running its jobs and processes and perform the copy task. HDFS to GCP2 is another open source to which is using Apache Nifi. It's similar to this cp2 that has some more features. Like it has its own validation mechanisms monitoring mechanism and also you can schedule restarts and
Presenter-A:  Let's talk resume the job if it's stopped at some point. So, it's much more advanced version, let's say. But installation of the software is a little bit more complicated. You need to consider more factors to work with this one. I like the CP, which is much more simple. If your working with Non-hdfs data, it's much simpler, which means that you examples are, you have just two here, that's you're not limited. You can use any any available to copy data, from simple file system to GCS, like, Cloud Source, Transfer, Service, or Gcuq tool. So, going forward.
Presenter-A:  We will talk about preparation work number two. So, At this point, we can say that we have already data moved to Jesus brackets. And we can perform some more cleanups, like, along with actual data files that we have copied. We can have some redundant files. Which in this case for example I put meta the data and the score seed or seed underscore version files which keeps on metadata information about hive tables which definitely notes.
