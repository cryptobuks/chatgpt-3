Presenter-A: Screen 20. So actually is either more than we usually use with everything that is.
Presenter-A:  Okay. So I have to bigquery migration. That's a kind of popular topic and popular use case that some of us probably have already faced and some of us going to face the future. Hive is Data Warehouse developed by Apache, which has quite popular among companies, Bigquery is kind of counterpart for hive that Google created and provides in GCP and which extends and adds a lot of more features. And usually while working with clients project or regarding migration, we have cases where we have to migrate data from, I have to be career. So of course, the scenarios and cases going to be a lot.
Presenter-A:  Based on the specific project but we will try to talk to a little bit general about use cases trying to cover most of the issues that you can face during this kind of project. And what's the possible?
Presenter-A:  Strategies Technologies for instep that you can use. So let's start by looking at general architecture for our case, How we're going to do this, migration. First of all, we are going to do some assumptions. So specifically, we will assume that net forking part regarding connection is already sold. So we don't have any connection issues. There are connection from our source environment to destination environment. Also there is no permission issues so we can go ahead. As you can see here, the brief architecture source environment, which in our cases, on PREM environment, we have hive database and it starts for me, then it goes to some preparation work first, preparation work.
Presenter-A:  After that, we use dataproc service as kind of staging environment and also compute for performing them migration tasks. We use this CP software later. We will talk a little bit more detail about this one. To get data from our source and copy to cloud storage after that, we have for us to for preparation work. And after that, we load the data into staging tables in Bigquery we again, do some modifications well loading data from staging to target tables. And after all of these we can say we that we have down the migration
Presenter-A:  So moving forward, let's start talking about the source. So as mentioned in our case, the source is Apache. Have a database so we can have Two Types of Tables. There are internal and external life tables. Each one has its specifications. As you can see regard, if you are working with internal tables,
Presenter-A:  Hive moves, stable data to data warehouse directory, and you have acid transactions enabled? It supported from for Truecade their other features like query result, caching works in this case. I like, in case of external type tables, and also dropping delete table data and metadata, like, in case of external table, where you even when delete table under link data still remains, and there is no support for truncate or acid transactions. so there could be different formats for source data, and each format actually has its specifications
Presenter-A:  And pros and consent while working with this format. So there I have mentioned here, the six main formats that usually you can face while working with Hive a CSV. Most common format just song, XML overall parquet and orc formats are more regarding the internal tables and those are preferred actually when you are important data to be query because those are usually in compressed format and makes it quicker and easier to work. Also, those usually have schema in file as well. So you don't need to worry about the schema of the destination table too much with the content for being able to
00:05:00
Presenter-A:  The data. So if we are looking, for example, at target, we there are some suggestions and requirements that you need to follow while working with
Presenter-A:  migration, this kind of format like Keep Record size to 50 megabytes or less and they're only specific compression Codex for data blocks in market files that supported by Bigquery. So you need to confirm that you follow those rules. And after that, you can go ahead and do the migration. So I have added all these details in the document that later I will share with you other. You can see all the details about each format
Presenter-A:  Which will be careful, I believe. So, moving forward, let's talk about first fats for Preparation block. So when we're working with internal tables, where we have acid operations enabled, Who may need to deal with Table compaction. So when you have this feature enabled during the each transaction when you
Presenter-A: Are Replace AD or remove something from tables. You are going to end up with having extra files in a Hive data warehouse. Directories. A lot of files going to be edit during those transactions and have used to manage operations this way.
Miguel Izaga: Up.
Presenter-A: So after those AC IDE operations, if you try to read something from result table, it's goes through all those small files to give you the last result of the data, but it it's kind of problematic for us because when we are going to migrate this format, it's going to be problematic for Bigquery to import that format is not supported.
Presenter-A:  And in our case we can do one thing that's Compaction. Compaction is specific operation that you can run on source cable which will combine all those small files that I was talking about and create one base file which will make it much easier to copy to destination directories and eventually load to bigquery tables. Without any issues As I have talked previously some files like CSV. They don't have information about schema in data file itself.
Presenter-A:  Some loading features like PQ load have options to auto detect schema. Well, you're loading data to be query table, but that's not ideal option and it has its limitations and sometimes it cannot ideally understand and detects schema based on the content file and in those four. Avoiding Issues and Any confusions. You may need to have your separate, schema dump file before starting loading process, and creating those files can be one of the preparation books.
00:10:00
Presenter-A:  Also, you need to collect more information about about source tables. Like Do you have partitions class source in source, tables? Are you going to have that in a destination table or information? Like that's source name, and destination name of the tables are the same or you're going to rename in the destination site. So all of this information you can keep in one config file.
Presenter-A:  Which will be part of the migration and I will, we will make it automatic later. I will show you some examples. And also one preparation work You need to collect information about existing processes and jobs that are running on source site because adding extra load by performing the migration called Create some conflicts with existing processing and jobs. And you may need, for example, some freeze time that you need to consider and plan. So all of these will be parts of the preparation work one.
Presenter-A:  So moving forward, this is just an example of config file, formats that in our case, we are going to use. So very simple. You just have table name bigquery data set name table name on destination site. The file format You need to preserve the schema file if you have such otherwise it will go by default scenarios of loading. And also you can specify if you have partition flag or not. Going forward. Let's talk about the next step, which is coping the actual data from source to GCS market, GCS bracket going to be a staging area, which going to be used. For actual load to Bigquery.
Presenter-A:  So there are different research. IT scenarios and tools that can be used for performing this operation. Most popular scenarios are the ones that listed in this screen. So if you are working with HDFS data, you can use one of the four tools mentioned here. GSU team to is not preferable to because in this case, it means that you need to copy from HDFS to local file system first and from there, copy to GCS which means if you have huge amount of data, it's going to be problematic to keep all the information in all the data and one file system before moving to
Presenter-A:  GCs, so it's mostly preferred option if you are working with a small amount of content. This CP tool is open open source software. Which can use can be used to copy data directly from HDF between HDFS and GCS.
Presenter-A:  So there are two methods to models push model and pool model. So in our scenario we will talk more about a pool model because it has more advantages, but there's also push model. So what we mean by push and pull models, DCP tool needs to be installed in the environment where you are going to run actual DCP command. So if you're going to push model, your you need to install the cp2 in directly, in source server, where you have your HDFS data located and around Command Director there, assuming that you have all the connection that access points enable,
Presenter-A:  And around the jobs to copy data from source environment to GCS Bucket. The one important disadvantages that if you are working with huge amount of data, Putting extra pressure on sourcing environments, or servers can affect existing jobs and existing processes that are running. So you need to consider that pool model. In other hand means that you are going to use third environment. Like we did, we are using data protein environment and there we are running DCP tool which will establish connection between source environment where actual HDFS data located and the destination environment GCS.
00:15:00
Presenter-A:  So it will run in dataproc and will not affect any. How To source environment which are running its jobs and processes and perform the copy task. HDFS to GCP2 is another open source to which is using Apache Nifi. It's similar to this cp2 that has some more features. Like it has its own validation mechanisms monitoring mechanism and also you can schedule restarts and
Presenter-A:  Let's talk resume the job if it's stopped at some point. So, it's much more advanced version, let's say. But installation of the software is a little bit more complicated. You need to consider more factors to work with this one. I like the CP, which is much more simple. If your working with Non-hdfs data, it's much simpler, which means that you examples are, you have just two here, that's you're not limited. You can use any any available to copy data, from simple file system to GCS, like, Cloud Source, Transfer, Service, or Gcuq tool. So, going forward.
Presenter-A:  We will talk about preparation work number two. So, At this point, we can say that we have already data moved to Jesus brackets. And we can perform some more cleanups, like, along with actual data files that we have copied. We can have some redundant files. Which in this case for example I put meta the data and the score seed or seed underscore version files which keeps on metadata information about hive tables which definitely notes.
Presenter-A:  Right expected format for performing bigquery load and will cause errors and conflicts while running our operations for loading data to Bigquery. So we need to get rid of these files and remove them. Also in CSV files, there could be some unexpected characters which usually are not recognized by the loaders. We need to
Presenter-A:  Manage those. We need to make sure that we have all the expected charcoals as well before starting load. Otherwise you are going to have issues and errors. XML format is preferred to convert to JSON. Unless this conversion is another problem, but converting it to Justin makes it much easier for loading to bigquery and avoiding any problems with XML because this is kind of not expected and not the best.
Presenter-A:  The format for loading to bigquery, a better to use JSON and later, we will see some helpful tools that's, I will share, which can be used for performing this conversion. So going forward, We will talk about actual loading to be query before starting loading data to Bigquery, we will need to create a data set where the table is going to be located and start loading first. We will load data to staging bigquery tables because usually loading directly to target table is not preferred as
00:20:00
Presenter-A:  Sometimes we can have some redundant and extra columns in the table after loading, which we need to get rid of before having our final form. So we prefer to have data loaded first into staging tables. And after that, use some queries to generate our final result and load to final tables, loading to Bigquery, also has different options. We can use low jobs that are provided by Bigquery service and can work with different kind of formats. Also, we can use just SQL operations like LOW DATA. SQL statement to load data from one or more files into
Presenter-A:  School or Existing Table Bigquery data transfer service is another option.
Presenter-A:  So this this is another feature that can be used, which again underlying users Load Jobs. Bigquery Storage, Right? API is a way for directly talking with API and performing loads. Using the more programmatic approaches like using Python or some other languages. So the options are different and there are many standards to do. So, after loading, we can go ahead and do some cleanups, like cleaning before before doing the cleanup. Actually, we need to do a load to final tables. So loading to final table will go along with removing
Presenter-A:  Extra Columns by query that are created As a result of the load after loading to final table. We need to perform validation. So we will use Google's Recommended Data, Validation tool, which is again open source software. That's very, very easy to use. It's just make sure that you have connection with two parts. between source and destination and it gives you opportunity to perform different kind of built-in and custom validations based on your queries like
Presenter-A:  Number of rows, schemas of the tables Specific aggregation queries that you can use to validate data between source and destination and many many more that we can do after validation and confirmation that everything is fine. And we are fine with the result. We can go ahead and do our cleanups for staging tables and staging files. So to clean up, we can say that we are done.
Presenter-A:  It, it sounds very quick but during this step flow, a lot of problems appear and we need to be attentive and consider all the possible problems. So going forward. Now we will talk and talk a little bit to the scripts which are used for this migration. So let me switch to another screen and show the scripts. Yeah. So scripts, also located in the repository that's available for everyone, I believe and the link will be shared so everyone can use
Presenter-A: Now I will show from private repository, the scripts. So therefore scripts actually that I will talk about the main script is have to be query data, migration script. So this is bar script that's used and the main arguments that I use here. So before starting, I will not go much to the details. Just will describe the main arguments that are needed for the scripts and also the main steps that's a required. So here,…
00:25:00
Jimmy Cui: You.
Presenter-A: what you need to specify, you need to specify source HDFS, address destination gear, GCS bracket, the Region Zone profile. For the configuration that supposed to have the needed, permissions to perform the operations, the network, which means VPC name, where you are going to perform more needed operations, the config fab path which going to be used the mode of the of running the script, the log file location and that's it. If we go to the down to the main function we will see main steps for running the script. So as mentioned it has two modes, local and remote. You used to start the script with local mode first.
Presenter-A:  It will create a Worker Cloud Data Pro cluster for you. Then it will copy script and configuration file to worker cluster and launch cluster directly there with remote mode. When you run it removed mode, it will go through the configuration file line by line and copy Table files by first tab for each. Align, and
Presenter-A:  After copying table files, it will perform a table data. Preparation, create a Bigquery data set and load date table from GCS to bigquery After this. It will perform some low consolidation between local and remote log files and remove worker cluster. So we will not have redundant class or running Summer in the cloud. So after this it will have the source tables created and source data loaded on destination side.
Presenter-A:  So this is the main script but we also have some helpful scripts here. Like I mentioned, I have added here, some Python script which will help if someone wants to load. The XML format to just format which as I mentioned, makes it much easier to row to bigquery there.
Presenter-A:  Have versus Bigquery validation script which again takes few arguments. You need to give source Have server to address destination Bigquery project. You need to give Yaml file, I pull, I already put some examples here in this directory so anyone can watch and find some helpful information. So in Yamo for format we define what kind of validation. Exactly, we need to perform and also log file, which will keep looks about this script. And the steps here, just two, it just adds Connections for source and destination for our data validation tool. First of all, it assumes that you have this data validation software installed and then it runs
Presenter-A:  Validation.
Presenter-A:  And also, I have helpful functions that by script here, which again keeps information about some helpful functions which can be used like, well you're working with source Datas I mentioned. Sometimes you may need to have TAB schema files which can be used for loading in case if you have problems with automatic schema detection. So you can use this one against source, this one will give will help you to get the partition information from source tables. Also the source back at column information. This one will help you for
Presenter-A:  Loading to Target Tables in Bigquery because the main script that I was showing before, it's responsible only for loading to staging tables. So this one will help you to get rid of two redundant tables are detended columns from staging table and run, SQL queries against staging table to row, to destination tables and have the final result again, some functions for data, validation for adding connections, for Generating config files, so you can
00:30:00
Presenter-A:  Use Different Features of Data Validation, Just directly in comment line, I have just added functions for some most popular options that you may need. So the functions can be helpful and also, This one will be responsible for creating the correct table on Bigquery site. If you want to keep your data, validation result directly in Bigquery, which is also possible. When you're working with data validation software,
Presenter-A:  So yeah, basically, those are the scripts that I wanted to show you in the report. You can see some more helpful information. You can see the config files which also can serve as a hint by working on the, this kind of project. So, this is some example, this is example of result, schema table, which you can use for creating table on a Bigquery, And more config files there. Yeah, going back to the presentation. so,
Presenter-A:  I have added here, references to GIT repository to TDD document, which has the detailed. Description about each step about each problems that can be faced during the process, which we have talked briefly. So, we'll be more helpful to go over document if you're planning to work on this kind of projects. Yeah, and I think that's it. Yeah, thank you for listening and if you have any questions, I will be happy to listen.
Jimmy Cui: Yeah, we are running out of time. Let's just one quick question. If anyone has Arthur.
Arthur Baghdasaryan: oh no, sorry it was just
Jimmy Cui: Okay.
Jimmy Cui:  Let's call any question from anyone.
Presenter-A: You are.
Arman Malkhasyan: I wanted to tell a great thank you. There are because it was very interesting and
Presenter-A: I believe. Into that track.
Presenter-A:  I, Thank you.
Jimmy Cui: Yeah, I see the content is very solid. Thank you very much for the sharing. And I believe this is your first presentation, pretty good. Look forward to your next sharing. Okay, let's move. The second topic. Mike will share the service control. It's your turn but we have to go faster.
Presenter-B: Oh, thanks for putting me on the spot. All right, let me figure out how I can share my screen here. This is gonna take a little while. Okay, let me know if you guys can see my screen. You should see a presentation on VPC service controls.
Jimmy Cui: Yeah. It's coming.
Presenter-B: All right, I'll I'll cancel my video since we don't need that. All right, So this talk is about VPC service controls and what they do, what are their cute concepts? How to apply them and special considerations when using them? I got the idea on the stock based on a recent client engagement and wanted to share my experience with the service and some of that fitfalls you may encounter when using it.
Presenter-B:  We start, well, if key concepts, you know why? And you know, what VPC service controls are any special gut just when working with them, and I'll also do a demo or actually two of the demos if I have time. So VPC, Service controls, it's a service from Google. That doesn't really have much to do with VPC, but has a lot to do with the controls around, Google Services and APIs. You know, Google Services are basically any service that you use through Google Console or through Terraform or through the Google SDK or even the G Cloud command. all of these tools basically talk to
00:35:00
Presenter-B:  Talk to a Google APS on the back end. And you know, they let you interact through a unify API with your cloud resources. Vpc, service controls. Give you ability to secure access to Google Cloud services. For example, are you know any services, such as storage services like Google Storage or Bigquery Compute, Engine gke, anything you'll use in GCP for the most part can be secured using VPC, service controls. There is a couple of considerations for various services. Google has a really big page about like every single service and it any issues with VPC service controls and you got us. I'll paste the link to that.
Presenter-B:  A website at the end of this presentation. But basically, you can use that as a reference to see whether the services protected or supported by VPC service controls. And if there are any issues, if you do protect that service in VPC service controls, Obviously in GCP, you can use Google im to control access to Google Services Service controls on another hand. Give you another way to control access to those APIs and the data that's stored behind them. For example, with VPC, service controls, you can deny users within the cervix perimeter. You can stop them from copying data from inside of the perimeter to outside of a perimeter. A good use case is for example, in case you have some PII data on a GCS bucket and you want to stop your users from being able to copy that data.
Presenter-B:  To a third party bucket outside of the perimeter. So basically this lets you protect your resources and your data you do this by creating a service parameter where you specify your projects and vpcs and which services are going to be protected in those perimeters. It's an opt-in service where you specify, which services, you want to protect and secure in case you want to protect all the services, you have to select all of them. But in a nutshell, they give you access to control. And limit who can access and where they can do it from. You know, in contrast to I AM or identity and access management, I am as an identity based access control system where as VPC service controls are context-based access control.
Presenter-B:  So if I am, you know, you have your identities, you know, service account, or my user email address is an identity with. I am, I can grant permissions to specific services to that identity. In contrast, the VPC service controls. I can control where the access can come from and where it can go to. So basically in a nutshell VPC, service controls is an API gateway to Google, APIs. That gives you an additional layer of protection above im
Presenter-B:  Okay. What benefits do I get out of this? Well, first off, we get, we avoid data exfiltration. We can disable copying or protected data to outside GCS Buckets, you know, let's say, you have a service account that is being used by Compute, instance, in your project. Are you can basically, deny any service account from copying data to third-party buckets outside of your service perimeter. And with that in mind, you can also use it to Stop stolen, JSON keys from being used outside of an authorized networks outside of a perimeter.
Presenter-B:  So basically, I'd say, you know, you somebody steals your service accounts, Jason Key and tries to use it on their workstation outside of the perimeter. Well, they're gonna get this message that says, Hey, you know, request is prohibited by your organizations policy, you know, you can't do this.
Presenter-B: In addition to that you can use access levels to permit access from outside networks. For example, let's say, you know you want to allow access to a terraform service account from a specified outside cider range. That's entirely possible that way. You can ensure that your engineers use terraform, you know, using service accounts only from their workstations and not from anywhere else in the world. If you throw a little bit of money at Google, you also get access to beyond Core Enterprise, which will give you, MD Light, MDMA MDM like controls to GCP, where you can add additional controls, like GEO IP, you know, location blocking as well as, you know, Device. Blocking access controls to your access levels.
00:40:00
Presenter-B:  Additionally, you can prevent misconfigured over early permissive ion policies. You know, let's say you have an IAM policy that somebody messes up and they give you way too much access. Well, with VPC service controls. We can ensure that that access has a second check, you know, to ensure. I mean you're not doing what you're not supposed to be doing. Um Additionally, you get access context context aware rules where you can specify what identities and identity types and access your Google APIs. And you also get to do some monitoring where you can view in your lot, you know, Cloud blogging, what? Service accounts, try to access. What Protectedly, yeah, and you know where they got tonight?
Presenter-B:  Thank you. Okay. None key concepts. The first key concept is perimeters. That's where you specify your projects and BPC networks and APIs. Our perimeters can be dry, run or enforced, dry. Run is basically where excesses permitted, but it's still lugged according to your policy and force, is where we do actually actual like, protection of the services. There are two types of perimeters as well. I regular perimeter and a bridge perimeter, bridge perimeters are used when you want to connect multiple parameters together and allow access between them.
Presenter-B: Brush primers are actually not really recommended to be used anymore. Bulks actually recommend using ingress in ingress policies and rules to permit data movement between perimeters.
Miguel Izaga: Up.
Presenter-B: ah, the reason for that is with bridges, you basically allow All the resources in the perimeter to be accessible by another perimeter. With ingress and egress policies. You basically specify granular controls of who can access, you know your APIs. Access levels are basically an ability are a concept where you can allow external external resources access to your perimeter. So with that, you can specify service accounts. You know, your cloud email, addresses. external public iPS Like, you know, an IP of your workstation or your, you know, Net IP that you're accessing from the outside,
Presenter-B:  Audit and logging, that's pretty standard it all happens. Within cloud logging. There's a special resource. You just got to look up. They'll show you, you know, your recent blocks and denies. Ingress and Egress Rule. They're basically a granular way to permit access into your perimeter and permit access out of your perimeter. Ingress rules are going to be helpful when you want to permit someone from outside of your organization.
Presenter-B: Or even even within your organization access from outside of the perimeter, you know, for example, let's say I protect GCs in the perimeter. Nobody outside of my project or outside of, this perimeter will be able to access GCS, unless, you know, I had their IP address to the excess list or access level. and to the in ingress rule, With those rules. You can also specify identities other projects that can get in other VPC networks that can get in an accessory APIs. Are egress, rules refer to any excess that involves an API client or resource within the parameter and resources outside. Our service perimeter. examples include A cloud storage client within your perimeter that wants to copy data from a protected bucket. To a bucket outside of your perimeter.
Presenter-B:  so, that way you can control Data equals from GCS and ensure that. It's allowed to only permit it resources outside of a perimeter, definitely a very good way to just protect your, you know, PII data in your buckets and disallow any outside copies.
Presenter-B: Egress and ingress rules can be written in JSON yml. As well as through the Cloud console in Terraform Terraform has a specific language called HCl Hashicorp Whatever language that's used to write Tera from recipes and configurations. They have a couple of gotchas where you know it looks kind of like JSON when you write your policies but they don't use Collins. They use.
00:45:00
Presenter-B:  Equal signs for values in assigning values. So it's just one of the gut just to look for when you're writing your policies. But an example policy looks fairly simple from English rules you specify? You can specify what identity types can access it whether it's users or service accounts, what identities you can actually that can actually access that API from outside and what sources we can use. So as a resource, you can specify another project or another VPC network outside of your perimeter or an access level. an ingress to role is basically specified, what services, you're allowing your
Presenter-B:  Identities to access and what permissions they have. So you can specify you know, Google's cloud storage. You have so many different methods that users can do like Get Post list. All these restful methods are basically used by the Cloud console or Gcloud or Gsutile to interface with cloud storage so we can control which methods we can do. same thing with egress rules, you can specify who Can. Basically move data outside of your perimeter, you specify what identities, what service accounts and word that data can go.
Presenter-B:  Special Considerations, When Engineering. Think about what Google services in APIs that you want to protect and why you want to protect them think really hard. Why you want to do this. You know what use cases, you have to protect your data. Why do you want to protect it? Is there any data that you want to prevent from being Extilated? What if there is level? Who are you protecting it? From what exceptions you have? Things like that, you know, think about really what you're protecting and why you're protecting, you know, if you're going to secure Google Cloud storage. Think about why you're going to protect it. Is there specific type of data that you want protected and not? Be able to be, you know, uploaded anywhere else. Next up, when designing perimeters, make the resource organization, hierarchy, work for you, especially when you're deploying your immersive using terraform.
Presenter-B:  When you create a perimeter, you specify, what service is you want to protect as well as the projects and dpcs that you'll allow access to those? Services from You know, when using TERRAFORM, oftentimes you'll find yourself using data sources or variables or string variables to pests resource IDs, like projects or VPC networks. You know, depending how you Design Your Organization Hierarchy. Sometimes it may make sense to use a data source to automatically pull all the projects from a folder to protect versus. Specifying them manually by hand using, you know, strings. Depending, how you design your organization, hierarchy, it may make it easier for you to filter through the data sources and, you know, pull all the project numbers from folders. And I'll show you guys later more. You know what I'm talking about in my terraform code?
Presenter-B:  Enable private Google access to allow private connectivity from a VPC on PREM Network to Google, APIs and Services. Basically this slash dirty range that I have on the screen, it's a Private, not announced to the Internet Google Network.
Presenter-B:  That basically stays within the Google networks when you're sending traffic to it. So because of this you can control What? IPS your Google services are accessible from. If any of you are familiar with AWS, this is very similar to VPC endpoints where you could create an endpoint within your VPC network. That exposed a particular service, it works in a similar way, you basically keep your API traffic private and you don't send it out to the public Internet where it will get routed to Google.
Presenter-B:  Next up, that's something I discovered when I was actually deploying VPC service controls for a client. If you have a shared VPC, host project, Ideally, it should not be power part of a perimeter. Mostly because a shared VPC, host project will probably have multiple shared Vpcs in it. and once a project is part of a VP's VPC service controls perimeter, it can only be part of one parameter, not part of multiple parameters. And any VPC network that's part of a parameter will retain its protections. So let's say, you know, I have this host project out there and I have multiple Vpcs shared to each perimeter. Uh, if I were to make The host project. Part of a perimeter, I would not be able to share those Vpcs to other parameters.
00:50:00
Presenter-B:  it just isn't allowed if I don't make the host project power of a perimeter, I can then take those shared vpcs and make them part of Individual parameters. That way, any resources that run on those vpcs will have protections defined by the perimeter that they're attached to you know, I typically in a host project you really don't have anything running, you really shouldn't have any resources running on it other than you know your hosts in your shared VPC so I think it makes sense not to have it part of a perimeter but you know, definitely make those bpcs part of parameters and actually Google has a page on including VPC networks in a service parameters and they have Additional.
Presenter-B:  Gushes as well you know you can definitely add PPC networks and host projects to the same parameter. It's just those VPC networks will not be able to be part of different parameters. Um, and you know, in our client engagement, you know, we had one host projects, host project with multiple vpcs. We had to take it out of a perimeter because those vpcs needed to be part of different parameters.
Presenter-B: Next up, Locking down services for users. Can have hilarious resort results if you log down, you know, Google Cloud Storage servers and you don't specify any access levels if I go to Google Cloud storage in. the Cloud Console, I will not be able to see my buckets because My user session for the Cloud. Console is coming from a public IP from my workstation. We're not allowing my workstation as part of access level, therefore, I will not be able to see any buckets within that perimeter. And that goes for any service.
Presenter-B:  So when you create a perimeters, especially when you first start off with VPC service controls, I recommend that you add your workstations IP to an access level. So that way, you don't get locked out. And I spent a lot of hours trying to figure out why I couldn't see, you know, my cloud storage buckets until I realized that my IPv6 address kept changing every couple hours and I'll keep getting locked out. So, I ended up having to add aesthetic, mapping in my router to add, You know, to make sure that I have a static IPv6 address. Otherwise, I'll have to add, You know, my slash 64 IPv6 prefix to an access level. And by the way access levels, do support, IPv4 and IPv6 UBC price. Actually how much IPv6 is used these days especially with the Cloud console, and Cloud APIs.
Presenter-B:  Next up ipll list where I just referenced ingress and rigorous rules something I mentioned before, as well, when you're writing power form, and you're ingress and ingress rules, and terraform, make sure you use the equal signs versus the columns. I've had numerous mistakes where I mix them up and I spent too much time debugging stupid colons. Lastly used trb to double. Check your sanity. I haven't done it as part of my past engagement and I regret it I think they could have saved me some time. But used trb have them double, check your sanity. And, you know, learn from my mistakes VPC, service controls can be annoying. But once they're understood, they're not that hard to implement.
Presenter-B:  Uh, demo. So if you guys have time to hang out after this call, our after, you know, this time, I'm going to do to demos where I'm going to create a perimeter and protect GCs, and I'll try and access data from inside of the perimeter and from outside of the perimeter, And then, you know, I'll add inbound and outbound egress rules to provide access. And later on all should show, top form and how to do this using terraform. Google actually has a Module that you can use to. Create and manage service controls. In fact, I had to create a full request with a bug fix to that module yesterday. Because as it turns out, they support adding projects as part of a perimeter.
00:55:00
Presenter-B:  They didn't support adding Vpcs. Even though the API for Access Context Manager, actually supports it.
Presenter-B:  Ah, yeah. So if I go to like the service permits overview, you know, you specify your resources that you want to
Presenter-B:  Protect and, you know, you can protect projects and vpcs at this point. Anyway. Are you guys ready for a demo? Do you guys have time to hang out for a couple more minutes? All right, cool.
Jimmy Cui: Yeah, let's continue. You know you first saw my kind of make it we'll share the recording session. Let's go.
Presenter-B:  Cool, awesome. All right, let me make sure Mike Entire screen.
Presenter-B:  All right. Make it a little bigger. So VPC service controls are on organization resource, you will manage them from your organization console
Presenter-B: You have numerous modes and force try mode, I'll focus on and…
Miguel Izaga: Up.
Presenter-B: enforce mode, for the sake of time. I'm going to create a perimeter called protective perimeter. Will make it regular perimeter, we'll select a project that I want to protect.
Presenter-B:  I'll select the VPC network that will allow access from the perimeter. Talk to that perimeter. Gotta find my shared VPC network.
Presenter-B:  Next up also like services, I want to protect Google right away recommends that you protect storage services, you'll even see it here.
Presenter-B: Mostly, because that's the best. That's the most likely to get data exfiltration happen to you. This, you know, buckets are pain in the butt to protect, especially when you start growing insights, and you start having multiple identities start to access them. VPC Accessible services. So you can control which services can access the which services are accessible using the private Google access. That is that slash dirty network with the private Non-routable, addresses that are private to Google Network. Ah, by default. I just select all services. Access levels. You can specify which axis level will be, you know, we'll have access. so, any external Resources. English Policies in English Policies For now. I'll leave blank. Just to show you what happens when I protect the service using a perimeter.
Presenter-B:  so, one of my projects that protected in my perimeter, I have a bucket and you'll see that. Well, right now I can still actually that's the wrong bucket.
Presenter-B:  That's the right bucket.
Presenter-B:  I should shortly get the night access. Yeah. There we go. Sorry. The server was not able to fulfill your request. Why is that? Well, that's because I did not add an access level. So I'm not allowing my laptop, my workstation to get to it. So I'm going to create an access level, and I'll allow my IP address. Let's see. IPv4.
Presenter-B:  There's also I'm going to add IPv6.
Presenter-B:  Okay. We can also specify what cloud identities can. Access this as well as GEO locations. um, see
Presenter-B:  So yeah, you can also like specify your cloud identity. So for example, You only want users from your organization to be able to browse through your resources in the Cloud console? You can specify their user IDs or service accounts in here as well. In case you don't want to be using, you know, IP cider ranges. For example, in case it becomes a pain to manage since a lot of users may have the ACP addresses or dynamic addresses. Anyway. Let's go back to our perimeter, let's specify an excess level.
Presenter-B:  We'll save it. perimeter update can take up to five minutes to or sometimes even up to 30 minutes to Update. I've seen it that typically updates within like the five minute period. Sometimes even quicker go back to our bucket, refresh. And we should be able to See the bucket resources real quick. Yep. So we have this file that sits on the bucket. Now, let's try and access this file from our
01:00:00
Presenter-B:  Perimeter. So up here, I have an instance in one of the projects that are part of the perimeter. You'll see that I can copy data from that bucket but I will not be able to copy data outside of this bucket.
Presenter-B:  So that works. Now, let's see. Let's copy data from this bucket to a bucket outside of the perimeter.
Presenter-B: And you'll see that our axis is denied because I'm haven't defined any egress rows to allow access to, you know, from my permit to another outside of outside outside perimeter. So, if I go back to my instance,
Presenter-B:  I'm gonna copy, it's service account.
Presenter-B:  And I'm going to go back to our Through the mirrors. I'm going to add an egress rule that says, Hey, this guy can
Presenter-B:  copy stuff. And we can specify which projects we can talk to so this gives us a granular way of control of what identity can copy data from my perimeter to another project.
Presenter-B:  Like this. Let's make sure.
Presenter-B:  Okay, services. We can specify that. I can only talk to storage API. We're going to save this and hopefully within a couple seconds. Our copy command will work. And like I said, sometimes it takes a couple minutes eventual consistency.
Presenter-B:  Let's see. There we go. It's looks like it's starting to get some API access Because it was able to look up the object. And there we go. It took us maybe 20 to 30 seconds, but we were able to finally copy data, from perimeter, bucket to a bucket outside of the perimeter. Now, let's say I have a resource on the outside of perimeter, this instance on the bottom, it's in one of my projects that's outside of a perimeter. Let's say I want to copy buckets from a perimeter or you know, data from a perimeter.
Presenter-B:  And that should fail, and that's because we need and One sec.
Presenter-B:  And in this case, we'll need an egret ingress rule to allow an outside identity to get inside of this perimeter. So, I'm going to go to my second instance and pull up. It's service account. And this instance is outside of my perimeter and I want to be able to talk to my parameters API to get resources from its protected bucket. So once again, let's go back to our perimeter.
Presenter-B:  Create an ingress policy. Will specify our service account.
Presenter-B:  Project will specify that it's coming from our outside project.
Presenter-B:  And we can also specify a source in Ibpc network.
Presenter-B:  If we wanted to,
Presenter-B: and we can specify, you know,
Presenter-B:  Projects. Services. We only want care about cloud storage right now. Click Save. Hopefully. We'll be able to copy it bucket. You know, bucket data from our perimeter, to an instance, outside of our perimeter because we just edit the ingress rule.
Jimmy Cui: So clever Mark's standing. The ingress rule is allowing the access from outside of the parameter.
01:05:00
Presenter-B: Right. Correct.
Jimmy Cui: You have to pull the data. Okay? So the egress is allowing the data to come outside of the
Presenter-B: egress allows you to Allow the data movement, from inside of your perimeter, to the outside of your perimeter.
Jimmy Cui: Yeah, yeah,…
Jimmy Cui: that's it. Your crush.
Presenter-B: Yeah, and…
Presenter-B: ingress like you said, allowing outside to come into your perimeter, to talk to your APIs in your perimeter.
Jimmy Cui: So when we allow the ingress, then we also allow the data you…
Presenter-B:  Great.
Jimmy Cui: outside, right? I mean from the parameter to move outside of the parameter.
Presenter-B:  Well, you don't have to, but you could do that, you know, when you create an ingress role, it's it's a directional access, it's not bidirectional. So you'll have to have the outside the reverse way as well added as any response. Yeah.
Jimmy Cui: That's a similar, like the new the network denied allowed by default is by directional. Cool.
Presenter-B: Correct. Yeah. With bridges. You can actually have it by directional or, you know, goes both ways. Ingress, and ingress rules are definitely a lot more granular. Anyway, I'm gonna delete this parameter and…
Jimmy Cui:  Mm-hmm.
Presenter-B: I'll show you how to do this real quick through Terraform. because there's a couple of gachas, I ran into So terraform there is a VPC service controls module. I'll put it in the chat.
Presenter-B: And I'll show you how I'm using it with our SMB Cloud Foundation, the Cloud Foundation that everyone's familiar with this one. So recently as part of a client project, we deployed this foundation for a client but they had one caveat, They wanted to add perimeters. So we added the new folder Dash 8-8, where we define the perimeters. I prefer to add them towards the end. Once you have your projects, your folder's created Same thing, it helps you out. When you're removing VPC service,
Presenter-B:  To create an access level, there is a module called Access Level where you can specify your IP, subnetworks your users, and identities your regions. Basically anything that we did Under here. Next up, we can create the perimeter. There's a module for that and that module is specify your resources that you want to protect. So your services, your VPC networks, any restricted services that you want to protect in that perimeter and access levels. And then within that, you can specify your ingress policies as well as your ingress policies. But look. It ingress policies in terraform. They're basically JSON but instead of columns you're using equal signs to remember, don't do something like this.
Presenter-B:  Because it'll break your policy. Will not work, it'll break it. Make sure you're using the equal signs. You know, your sources here, you can specify your excess levels your resources. So the same thing that we, you know, when we created the perimeter, you know, when you created your ingress policies, that's where you can specify your identities or identity types. then you have your two stands out, where you specify which APIs. These resources, this these identities and these Resources can talk to. With egress policies, same thing, you know, what access, you know, what, service accounts can talk to the outside of the perimeter. So in this case, and I have a couple Service accounts that I'm selected. That can talk to these APIs outside.
Presenter-B:  So, now I'm gonna run this terraform plan. It's gonna run. It's going to show us that it's going to create perimeters ingress rules. I'm just going to apply this now and we'll do the review. Once it's applied, we like to live dangerously here.
Presenter-B:  Now, there's already an access policy, interesting. Default Policy, Let's delete the default policy. Not sure why it's in here and policies.
01:10:00
Presenter-B:  It's interesting.
Presenter-B:  And of course, I have to run into an issue, right? During my day, let's see here.
Presenter-B: I guess, let's Interest of Time, Let's just disable this resource.
Jimmy Cui:  So, how to run the initialization?
Presenter-B:  You. I don't think I should do a considering, I've Rent this multiple times before and I didn't change any modules underneath. Yeah, I'm not sure what created this policy. Can actually specify it is by name. Let's
Presenter-B:  Yeah.
Presenter-B:  Appellate your sex is Context Manager.
Presenter-B:  I'll see.
Presenter-B: Area.
Presenter-B:  Okay.
Presenter-B: Here. Yeah.
Presenter-B:  not sure why it wouldn't let me do it through my console pot. It's not a problem. It could be because of my cloud identity. I'm using my Sarah identity on A Organization, that's outside of Salah anyway. Plan.
Presenter-B:  Okay.
Presenter-B: Back.
Presenter-B:  That's a good sign.
Jimmy Cui: Yeah.
Presenter-B:  All right. Let's go back to our rumors. You'll see that in here.
Presenter-B:  Wrong window.
Presenter-B:  There we go. I've created two power. Two parameters. I have one protective perimeter where I've specified a bunch of projects, a bunch of VPC networks couple restricted services. And a couple ingress rules that basically allow any access from my access level. So, you know, my IP address. You know, let's say Jimmy if you want to this organization since you're not part of the access level, you would not be able to browse any of the Bigquery tables or cloud storage buckets. Within these projects.
Presenter-B:  In an egress rule. Basically, I'm allowing any identity to, you know, move data outside of this perimeter. Protected, sorry, wrong perimeter. And they have another perimeter with just one project and one VPC, But I have an ingress role where I allow a bunch of identities to access these resources. so, you know, my username or like my terraform service account or like these service accounts will be able to Go to. This perimeter and pull up these resources. So let's say, you know, I go to clouds storage. Under. Protected projects.
Presenter-B:  and here, you know, I can view my buckets since I there is an English rule that allows access cool. Any questions so far, any concerns any thoughts? Anything else that you wanted to see or play around within this demo?
01:15:00
Jimmy Cui: Yeah. Josh
Presenter-B:  Hey, George.
George Alonge: Thanks Mike. My question is the access context manager, can the IPS be private IPS? So here's the use case. Like, if I have a VPN connection to my GCP, um, GCP project, right? And then I walk and then I want to limit access within that within that connection. Are two particularly users.
Presenter-B: So excess control sorry access levels they only work with public iPS but VPN connectivity is supported. You can actually enable private Google access for on-premises networks as well by routes and route advertisements to through that VPN. So, basically, you know, you have your VPN between your GCP organization and your on-premises. In dead VPN, you would specify a route to the private Google access, which would extend your Perimeters or, you know, your access your perimeters to an outside. Network within Access Levels. I think you build believe it, you'll only specify your vpn's public. Gateway.
Presenter-B:  And then, you know, you're allowing access to ingress rules where you can specify, you know what networks can go through there as well. And what identities you just use routes? At this point to advertise that network out there. Yeah.
George Alonge: Oh, I see. Oh, because I had, we had a situation recently where we are configured the VPN connection and we did set up the VPN gateway on VPC service controls but then it wasn't working with the VPN connection. So, but I think what you said about the private Google access is that is that like an additional configuration that we need to do. Allow to allow that to work.
Presenter-B: Yes.
Presenter-B: yeah, I've pasted it in the chat as well and I believe, they also had a page on actually on-prem access to
Presenter-B:  Oh, from outside of a perimeter.
Presenter-B: Yeah, you should be able to I was just out personally, outlab it out in my org to see you like, What components and what gushes you're gonna run into. I personally haven't done it but I I've allowed only like my workstation X is not VPN, but it should be supported because I've seen it reference in their docs a few times.
Presenter-B: Yeah, make sure that you have Club VPN tunnel connection to support. Yeah, so it seems like it's supported it just threw me some gaches that, you know, we have to let it out.
George Alonge: Thanks.
Jimmy Cui: Yeah, I have two questions Mark, let's say I'm a super user for one organization…
Presenter-B: Yeah.
Jimmy Cui: but still, I want to restrict my access to one production GCS bucket.
Presenter-B:  Yeah.
Jimmy Cui: So you are saying, I have to add explicit denying row of policy. To do that.
Presenter-B: No, you shouldn't. So let's say you want to protect your bucket, you put it in a project that you want to protect and you specify. Cloud storage that you want to protect. And once it's in the perimeter,…
Jimmy Cui:  Yeah.
Presenter-B: Anything inside of the perimeter will not be able to talk to cloud storage outside of the perimeter. And nothing from the outside of the perimeter, will be able to talk to the cloud storage bucket. In that perimeter. You'll then have to add ingress and egress rules to allow data transfer from in, or out to the perimeter.
Jimmy Cui:  so, my question is, Should we set up some explicit deny or by default? It is denied.
Presenter-B:  It by default, it is denied. Once you protect the service, Yeah.
Jimmy Cui: Oh okay. That's why you're enable this protection. Is denied,…
Presenter-B: Yes. Yeah,…
Jimmy Cui: okay. Unless you add some ingress right along.
Presenter-B: and it's it's an yeah. Correct. Yeah and it's an opt-in service. So if you don't specify a service in here, It'll be in a wide open. so, in order to deny,…
Jimmy Cui:  Mm-hmm.
Presenter-B: you have to edit to restricted services,
Jimmy Cui:  Yep. Oh, second question. Um, you know, we can restrict access By IP by whatever. You know the resources say you are using the jambox. What if we allow the access from the GEM box? We deny access from your local IP.
01:20:00
Presenter-B: Oh yeah, that's entirely possible. Here's how I do. I create an yeah,…
Jimmy Cui:  I mean, you know,
Presenter-B: it's it's possible. So basically. you don't in ingress rule, you'll specify, you know, the project or a VPC network or you know identity Where that jump box is running. but you won't specify an access level, for example, in this case,
Jimmy Cui: Yeah, let me get more clear, say from the jambox,…
Presenter-B:  Yeah.
Jimmy Cui: we have a loud service account. But I want this a lot of denied my own user accounts. So what if I access the Gem box, then from the Gem box are accessed. The resource.
Jimmy Cui:  Will that be allowed or denied? Because my user account is denied, but I'm using the Gem box, which is a lot.
Presenter-B: Correct. Yeah, you're going to be using that jump boxes are identity.
Jimmy Cui:  I mean, so myself, maybe that account is denied, but I can still use the gem box.
Presenter-B:  Correct, you could use that jump boxes identity. As long as that identity is part of the ingress rule. Yeah. Yeah. Correct.
Jimmy Cui:  Okay, so talk about the IP. If my public IP is denied but the jump box is allowed. So, connect your access from the Gem box.
Presenter-B: Correct. E You could log into that jump box and using that jump boxes identity. You know that service account you would be x be able to access the cloud storage bucket. But you would not be able to login as your own cloud user,…
Jimmy Cui: Yeah.
Jimmy Cui:  That's right.
Presenter-B: and Try using your identity from the jump box.
Jimmy Cui: So, it looks like we have some hiking way.
Presenter-B: You have to use it. Yeah.
Jimmy Cui: You know, to allow some,…
Presenter-B: Yeah. Correct.
Jimmy Cui: you know, restrict users. Right. Okay.
Presenter-B: Yeah, yeah, you can get very granular, I recommend actually creating a spreadsheet of your perimeters, what's inside of those parameters?
Jimmy Cui:  Hmm.
Presenter-B: What services, you want to protect, and what ingress and egress rules? You want to create for each perimeter? Um, and you can get very granular because you can basically specify only, these service accounts are able to talk to the API in the perimeter. And only these service accounts within the parameter are able to get the data out. It, you know, the sub data movement. Yeah and…
Jimmy Cui: Yeah.
Presenter-B: it's it's a really good way to basically let's say somebody steals your service account keys that you used to deploy using, you know, terraform you know, they could run those, you know, they could run terraform from their workstation, you…
Jimmy Cui: Right.
Presenter-B: using that service account keys. But with excess levels, I can specify that. Hey,
Presenter-B: This and you know, ingress rules in the parameters, you can only specific. You can specify that only this service account can come from. This allowed,…
Jimmy Cui:  Number box.
Presenter-B: yeah from a jump box, or from like your best and host or from your organizations, you know, allowed list IP addresses. So, it gives like another layer of protection above.
Jimmy Cui:  Yeah, makes sense.
Presenter-B: I am because I am, it's really about identities. What am I allowed to do in GCP here with VPC service controls. Okay, where can I come from to talk to the Google APIs? And what data can I get out once I'm inside of the perimeter?
Jimmy Cui: Uh-huh, probably one more question. Let's say I have a host project having three, shared Vpcs.
Jimmy Cui: I think in every service project, I can see those three Vpcs. Right. So…
Presenter-B: Correct.
Jimmy Cui: what if I want to restrict HEY for Service for Service Project A you can access VPC. A only can we do that by our service control?
Presenter-B:  Ah,
Presenter-B: You could I know you definitely can do that with im Because you're specifying. You know, what service account or identity can have access to that particular sub-network or VPC in a host project. You could add the VPC to be part of the perimeter.
Jimmy Cui:  Okay.
Presenter-B: So only resources in this VPC can talk to the APIs, in this parameter. So, any compute instance on this network would be allowed to talk to cloud storage in this perimeter.
Jimmy Cui: Yeah.
Presenter-B: if you had something in a different VPC, It would not be allowed in this case. I have to go in here and add the VPC in order for traffic to be able to to be allowed.
Jimmy Cui: Cool. Oh we have several members.
Presenter-B:  Yeah.
Jimmy Cui: Still on here. Any question from you? Let's
Presenter-B:  Well, thank you guys for your that's all good. Right?
Josh Penalosa: Not.
Josh Penalosa: I was just saying done for me. Thank you very much, Mike.
Presenter-B: Thank you, Josh. Yeah, thank you guys for holding on for, you know, half hour past meeting and, you know, listening to me talk about VPC service controls. Hopefully you learn something. You…
01:25:00
Jimmy Cui: Yeah.
Presenter-B: let me know if you have any questions. They're not that harder pain in the ass to work with, but once you understand them, once you start playing with them. They're not that hard things start to make sense. Especially once you ensure that you don't like yourself out of the service and by the way up, if you ever protect it with Strep project, In the foundation. Make sure that you're access levels allow access for terraform to get into the bootstrap project to update, you know, resources or like your remotes state back-end bucket because I've had that happen to me where I protected that Booth strip project, I protected the cloud storage.
Presenter-B:  Service, but I never added my public IP to the access level. So, I basically locked myself out of terraform for that organization and I had to go through the console and, you know, unlock myself So, just an FYI. Cool.
Jimmy Cui: Yep. Yep. Thank you so much. Mike for the one of our sharing. Yeah, we'll learn something from you.
Presenter-B:  Thank you. Thank you, appreciate.
Jimmy Cui: Cool. Thank you everyone for joy. I'll see you next time. Bye.
Josh Penalosa: Bye everyone.
Presenter-B:  Bye.
Presenter-A: Thank you.
Jonathan Moisan: Thanks. Bye.
Jimmy Cui:  Thank you.
Meeting ended after 01:26:28 👋