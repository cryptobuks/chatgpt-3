Presenter-A:  Okay. So I have to bigquery migration. That's a kind of popular topic and popular use case that some of us probably have already faced and some of us going to face the future. Hive is Data Warehouse developed by Apache, which has quite popular among companies, Bigquery is kind of counterpart for hive that Google created and provides in GCP and which extends and adds a lot of more features. And usually while working with clients project or regarding migration, we have cases where we have to migrate data from, I have to be career. So of course, the scenarios and cases going to be a lot.
Presenter-A:  Based on the specific project but we will try to talk to a little bit general about use cases trying to cover most of the issues that you can face during this kind of project. And what's the possible?
Presenter-A:  Strategies Technologies for instep that you can use. So let's start by looking at general architecture for our case, How we're going to do this, migration. First of all, we are going to do some assumptions. So specifically, we will assume that net forking part regarding connection is already sold. So we don't have any connection issues. There are connection from our source environment to destination environment. Also there is no permission issues so we can go ahead. As you can see here, the brief architecture source environment, which in our cases, on PREM environment, we have hive database and it starts for me, then it goes to some preparation work first, preparation work.
Presenter-A:  After that, we use dataproc service as kind of staging environment and also compute for performing them migration tasks. We use this CP software later. We will talk a little bit more detail about this one. To get data from our source and copy to cloud storage after that, we have for us to for preparation work. And after that, we load the data into staging tables in Bigquery we again, do some modifications well loading data from staging to target tables. And after all of these we can say we that we have down the migration
Presenter-A:  So moving forward, let's start talking about the source. So as mentioned in our case, the source is Apache. Have a database so we can have Two Types of Tables. There are internal and external life tables. Each one has its specifications. As you can see regard, if you are working with internal tables,
Presenter-A:  Hive moves, stable data to data warehouse directory, and you have acid transactions enabled? It supported from for Truecade their other features like query result, caching works in this case. I like, in case of external type tables, and also dropping delete table data and metadata, like, in case of external table, where you even when delete table under link data still remains, and there is no support for truncate or acid transactions. so there could be different formats for source data, and each format actually has its specifications
Presenter-A:  And pros and consent while working with this format. So there I have mentioned here, the six main formats that usually you can face while working with Hive a CSV. Most common format just song, XML overall parquet and orc formats are more regarding the internal tables and those are preferred actually when you are important data to be query because those are usually in compressed format and makes it quicker and easier to work. Also, those usually have schema in file as well. So you don't need to worry about the schema of the destination table too much with the content for being able to
