Presenter-A:  Right expected format for performing bigquery load and will cause errors and conflicts while running our operations for loading data to Bigquery. So we need to get rid of these files and remove them. Also in CSV files, there could be some unexpected characters which usually are not recognized by the loaders. We need to
Presenter-A:  Manage those. We need to make sure that we have all the expected charcoals as well before starting load. Otherwise you are going to have issues and errors. XML format is preferred to convert to JSON. Unless this conversion is another problem, but converting it to Justin makes it much easier for loading to bigquery and avoiding any problems with XML because this is kind of not expected and not the best.
Presenter-A:  The format for loading to bigquery, a better to use JSON and later, we will see some helpful tools that's, I will share, which can be used for performing this conversion. So going forward, We will talk about actual loading to be query before starting loading data to Bigquery, we will need to create a data set where the table is going to be located and start loading first. We will load data to staging bigquery tables because usually loading directly to target table is not preferred as
00:20:00
Presenter-A:  Sometimes we can have some redundant and extra columns in the table after loading, which we need to get rid of before having our final form. So we prefer to have data loaded first into staging tables. And after that, use some queries to generate our final result and load to final tables, loading to Bigquery, also has different options. We can use low jobs that are provided by Bigquery service and can work with different kind of formats. Also, we can use just SQL operations like LOW DATA. SQL statement to load data from one or more files into
Presenter-A:  School or Existing Table Bigquery data transfer service is another option.
Presenter-A:  So this this is another feature that can be used, which again underlying users Load Jobs. Bigquery Storage, Right? API is a way for directly talking with API and performing loads. Using the more programmatic approaches like using Python or some other languages. So the options are different and there are many standards to do. So, after loading, we can go ahead and do some cleanups, like cleaning before before doing the cleanup. Actually, we need to do a load to final tables. So loading to final table will go along with removing
Presenter-A:  Extra Columns by query that are created As a result of the load after loading to final table. We need to perform validation. So we will use Google's Recommended Data, Validation tool, which is again open source software. That's very, very easy to use. It's just make sure that you have connection with two parts. between source and destination and it gives you opportunity to perform different kind of built-in and custom validations based on your queries like
Presenter-A:  Number of rows, schemas of the tables Specific aggregation queries that you can use to validate data between source and destination and many many more that we can do after validation and confirmation that everything is fine. And we are fine with the result. We can go ahead and do our cleanups for staging tables and staging files. So to clean up, we can say that we are done.
Presenter-A:  It, it sounds very quick but during this step flow, a lot of problems appear and we need to be attentive and consider all the possible problems. So going forward. Now we will talk and talk a little bit to the scripts which are used for this migration. So let me switch to another screen and show the scripts. Yeah. So scripts, also located in the repository that's available for everyone, I believe and the link will be shared so everyone can use
Presenter-A: Now I will show from private repository, the scripts. So therefore scripts actually that I will talk about the main script is have to be query data, migration script. So this is bar script that's used and the main arguments that I use here. So before starting, I will not go much to the details. Just will describe the main arguments that are needed for the scripts and also the main steps that's a required. So here,â€¦
00:25:00
Jimmy Cui: You.
Presenter-A: what you need to specify, you need to specify source HDFS, address destination gear, GCS bracket, the Region Zone profile. For the configuration that supposed to have the needed, permissions to perform the operations, the network, which means VPC name, where you are going to perform more needed operations, the config fab path which going to be used the mode of the of running the script, the log file location and that's it. If we go to the down to the main function we will see main steps for running the script. So as mentioned it has two modes, local and remote. You used to start the script with local mode first.
Presenter-A:  It will create a Worker Cloud Data Pro cluster for you. Then it will copy script and configuration file to worker cluster and launch cluster directly there with remote mode. When you run it removed mode, it will go through the configuration file line by line and copy Table files by first tab for each. Align, and
Presenter-A:  After copying table files, it will perform a table data. Preparation, create a Bigquery data set and load date table from GCS to bigquery After this. It will perform some low consolidation between local and remote log files and remove worker cluster. So we will not have redundant class or running Summer in the cloud. So after this it will have the source tables created and source data loaded on destination side.
Presenter-A:  So this is the main script but we also have some helpful scripts here. Like I mentioned, I have added here, some Python script which will help if someone wants to load. The XML format to just format which as I mentioned, makes it much easier to row to bigquery there.
Presenter-A:  Have versus Bigquery validation script which again takes few arguments. You need to give source Have server to address destination Bigquery project. You need to give Yaml file, I pull, I already put some examples here in this directory so anyone can watch and find some helpful information. So in Yamo for format we define what kind of validation. Exactly, we need to perform and also log file, which will keep looks about this script. And the steps here, just two, it just adds Connections for source and destination for our data validation tool. First of all, it assumes that you have this data validation software installed and then it runs
Presenter-A:  Validation.
Presenter-A:  And also, I have helpful functions that by script here, which again keeps information about some helpful functions which can be used like, well you're working with source Datas I mentioned. Sometimes you may need to have TAB schema files which can be used for loading in case if you have problems with automatic schema detection. So you can use this one against source, this one will give will help you to get the partition information from source tables. Also the source back at column information. This one will help you for
Presenter-A:  Loading to Target Tables in Bigquery because the main script that I was showing before, it's responsible only for loading to staging tables. So this one will help you to get rid of two redundant tables are detended columns from staging table and run, SQL queries against staging table to row, to destination tables and have the final result again, some functions for data, validation for adding connections, for Generating config files, so you can
00:30:00
Presenter-A:  Use Different Features of Data Validation, Just directly in comment line, I have just added functions for some most popular options that you may need. So the functions can be helpful and also, This one will be responsible for creating the correct table on Bigquery site. If you want to keep your data, validation result directly in Bigquery, which is also possible. When you're working with data validation software,
Presenter-A:  So yeah, basically, those are the scripts that I wanted to show you in the report. You can see some more helpful information. You can see the config files which also can serve as a hint by working on the, this kind of project. So, this is some example, this is example of result, schema table, which you can use for creating table on a Bigquery, And more config files there. Yeah, going back to the presentation. so,
Presenter-A:  I have added here, references to GIT repository to TDD document, which has the detailed. Description about each step about each problems that can be faced during the process, which we have talked briefly. So, we'll be more helpful to go over document if you're planning to work on this kind of projects. Yeah, and I think that's it. Yeah, thank you for listening and if you have any questions, I will be happy to listen.
Jimmy Cui: Yeah, we are running out of time. Let's just one quick question. If anyone has Arthur.
Arthur Baghdasaryan: oh no, sorry it was just
Jimmy Cui: Okay.
Jimmy Cui:  Let's call any question from anyone.
Presenter-A: You are.
Arman Malkhasyan: I wanted to tell a great thank you. There are because it was very interesting and
Presenter-A: I believe. Into that track.
Presenter-A:  I, Thank you.
Jimmy Cui: Yeah, I see the content is very solid. Thank you very much for the sharing. And I believe this is your first presentation, pretty good. Look forward to your next sharing. Okay, let's move. The second topic. Mike will share the service control. It's your turn but we have to go faster.
Presenter-B: Oh, thanks for putting me on the spot. All right, let me figure out how I can share my screen here. This is gonna take a little while. Okay, let me know if you guys can see my screen. You should see a presentation on VPC service controls.
Jimmy Cui: Yeah. It's coming.
Presenter-B: All right, I'll I'll cancel my video since we don't need that. All right, So this talk is about VPC service controls and what they do, what are their cute concepts? How to apply them and special considerations when using them? I got the idea on the stock based on a recent client engagement and wanted to share my experience with the service and some of that fitfalls you may encounter when using it.
